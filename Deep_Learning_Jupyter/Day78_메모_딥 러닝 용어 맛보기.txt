
은닉층(Hidden Layer)이 있으면 딥 러닝으로 볼 수 있다.


[ 경사하강법 ]

학습률 : 경사를 내려오는 보폭 (속도)
            --> 너무빨리 내려와도, 느리게 내려와도 좋지 않다.


풀 배치 : 전체 데이터를 사용 (하나하나 다 뽑아서 오차 확인)
	- 문제점 : 시간이 너무 오래 걸린다



[ Propagation ]

순전파 : 순차적으로 데이터 

역전파 : 실제 결과값과 얻은 결과값의 차이인 오차를 이용
	오차를 활용하여 이 전에 작업한 곳으로 이동하여 가중치 조정.







[ 합성 곱 신경망 (CNN) ]
- 주로 이미지 처리를 위해 사용

이미지에 대한 학습 방법을 설명
- 합성곱 -> 풀링(이미지를 줄여주고) -> 평탄화작업(2차원을 1차원으로)



1) 합성곱 계층
- 주요 특성(특징)들만 활용
- 필터를 여러개 사용

Padding(패딩)
- 데이터를 감싸는 테두리
- 패딩을 사용하면 주요 특성을 더 활용이 가능.(정확도 올라갈 듯 싶음)
- margin : 패딩과 패딩 사이의 간격을 의미


2) Pooling(풀링)
- 특성맵 크기를 줄임
- 특성을 대표할 수 있는 값(특성의 개수를 줄일 수 있을 듯)
- 최대값 or 평균값을 활용


3) 전결합
- 2차원을 1차원으로 펼침





[RNN]

값 이 입력됨.
입력된 값을 연산하고 연산 후 그 출력된 값을 다시 내가 받음 
다음 값이 들어오면 앞서 출력된 결과를 새로운 값과 함께 연산하고 결과 출력
다음 값이 들어오면 앞서 출력된 결과를 새로운 값과 함께 연산하고 결과 출력
모든값이 들어올때까지 반복....

다 들어오면 다음 단계로 이동
(은닉층 1층 완료후 2층 3층...)

ex)
my name is hgd 입력 시
-> my,  my name, my name is, my name is hgd


문제점: 장기의존성 문제 (기억 잃음 --> 기울기 0이 되어버림)
- 문장이 길어질수록 앞 단어 정보를 잃음 (my를 까먹을 수 있다)
- 의미가 없어질 수 있다. (오랫동안 기억할 수 없다)




[ Dropout ]
- 특정 뉴런을 사용하지 않도록 (의미없는 뉴런)
- 쓸데없는 요인은 안쓰게 함




[ Momentum ]
- 가속도를 의미
- 경사를 내려오는 가속도?




































